---
monitoring_localuser: ubuntu
monitoring_localgroup: ubuntu

prometheus_server_docker_tags: 'v2.37.1'
prometheus_server_title: SKA DevOps Prometheus Server
prometheus_server_memory: 16384M
prometheus_server_swapfile_path: /swapfile
prometheus_server_swapfile_size: 1000
prometheus_server_swapfile_fallocate: true

certificates_dir: /etc/pki/tls/private

prometheus_alt_name: "monitoring.skao.stfc"

# vars file for prometheus
prometheus_gitlab_ci_pipelines_exporter_tags: v0.5.3
prometheus_gitlab_ci_pipelines_exporter_token: "{{ _ | mandatory('`prometheus_gitlab_ci_pipelines_exporter_token` definition is mandatory') }}"
prometheus_retention_time: "15d"

prometheus_server_kubectl_version: 'v1.22.5'
prometheus_server_kubectl_name: kubectl
prometheus_server_kubectl_install_dir: /usr/local/bin
prometheus_server_kubectl_url: 'https://storage.googleapis.com/kubernetes-release/release/{{ prometheus_server_kubectl_version }}/bin/linux/amd64/{{ prometheus_server_kubectl_name }}' # noqa yaml[line-length]

prometheus_k8s_api_server_port: "6443"
prometheus_k8s_api_server: "https://{{ prometheus_k8s_api_server_addr }}:{{ prometheus_k8s_api_server_port }}"
prometheus_k8s_client_certificate: "/etc/prometheus/ca.crt"
prometheus_k8s_bearer_token: "/etc/prometheus/bearer.token"

prometheus_openstack_exporter_addr: ""

prometheus_server_config_dir: /etc/prometheus
prometheus_data_dir: /var/lib/prometheus

prometheus_node_metric_relabel_configs: []
prometheus_static_node_metric_relabel_configs: []

prometheus_elasticsearch_instance_add_cluster_label: []

prometheus_server_playbook_dir: /usr/src/deploy-prometheus/
prometheus_gitlab_url: git@gitlab.com
prometheus_repo_base: "{{ prometheus_gitlab_url }}:ska-telescope"

prometheus_global:
  scrape_interval: 15s
  scrape_timeout: 10s
  evaluation_interval: 15s

prometheus_remote_write: []

prometheus_remote_read: []

prometheus_alertmanager_config:
  - scheme: http
    static_configs:
      - targets: ["{{ ansible_fqdn | default(ansible_host) | default('localhost') }}:9093"]

prometheus_scrape_configs:
  - job_name: "prometheus"
    scheme: https
    tls_config:
      insecure_skip_verify: true
    static_configs:
      - targets:
          - "{{ ansible_fqdn | default(ansible_host) | default('localhost') }}:9090"

  - job_name: "gitlab_ci_pipelines_exporter"
    static_configs:
      - targets:
          - "{{ ansible_fqdn | default(ansible_host) | default('localhost') }}:8080"

  - job_name: "node"
    file_sd_configs:
      - files:
          - 'node_exporter.json'
    metric_relabel_configs: "{{ prometheus_static_node_metric_relabel_configs +
       prometheus_elasticsearch_instance_add_cluster_label + prometheus_node_metric_relabel_configs }}"

  - job_name: "gitlab-runner"
    file_sd_configs:
      - files:
          - 'gitlab_exporter.json'
    metric_relabel_configs: "{{ prometheus_static_node_metric_relabel_configs +
       prometheus_elasticsearch_instance_add_cluster_label + prometheus_node_metric_relabel_configs }}"

  - job_name: "elasticsearch"
    file_sd_configs:
      - files:
          - 'elasticsearch_exporter.json'

  - job_name: "docker"
    file_sd_configs:
      - files:
          - 'docker_exporter.json'

  - job_name: "docker_cadvisor"
    file_sd_configs:
      - files:
          - 'docker_cadvisor.json'
    metric_relabel_configs: "{{ prometheus_static_node_metric_relabel_configs +
       prometheus_elasticsearch_instance_add_cluster_label + prometheus_node_metric_relabel_configs }}"

  - job_name: "kube-proxy"
    file_sd_configs:
      - files:
          - 'kube-proxy.json'
    metric_relabel_configs: "{{ prometheus_static_node_metric_relabel_configs +
       prometheus_elasticsearch_instance_add_cluster_label + prometheus_node_metric_relabel_configs }}"

  - job_name: "kube-state-metrics"
    static_configs:
      - targets:
          - "192.168.93.102:32080"  # load balancer
    metric_relabel_configs:
      - action: replace
        regex: 192\.168\.93\.102:32080
        replacement: k8s-syscore-loadbalancer-0:32080
        source_labels:
          - instance
        target_label: instance

  - job_name: "k8stelemetry"
    static_configs:
      - targets:
          - "192.168.93.102:32081"  # load balancer
    metric_relabel_configs:
      - action: replace
        regex: 192\.168\.93\.102:32080
        replacement: k8s-syscore-loadbalancer-0:32080
        source_labels:
          - instance
        target_label: instance

  - job_name: "ceph_cluster"
    file_sd_configs:
      - files:
          - 'ceph-mgr.json'

prometheus_blackbox_ssh_targets:
  - "{{ ansible_fqdn | default(ansible_host) | default('localhost') }}:22"  # matteo devenv

prometheus_server_record_rules:
  - record: instance:node_cpu:load
    expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)

  - record: instance:node_ram:usage
    expr: (1 - node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes) * 100

  - record: instance:node_fs:disk_space
    expr: node_filesystem_avail_bytes{mountpoint="/",job="node"} / node_filesystem_size_bytes{mountpoint="/",job="node"} * 100

prometheus_server_alert_rules:
  - alert: Watchdog
    expr: vector(1)
    for: 10m
    labels:
      severity: warning
    annotations:
      message: 'This is an alert meant to ensure that the entire alerting pipeline is functional.
        This alert is always firing, therefore it should always be firing in Alertmanager
        and always fire against a receiver. There are integrations with various notification
        mechanisms that send a notification when this alert is not firing. For example the
        "DeadMansSnitch" integration in PagerDuty.'
      summary: 'Ensure entire alerting pipeline is functional'
  - alert: InstanceDown
    expr: "up == 0"
    for: 5m
    labels:
      severity: critical
    annotations:
      message: "{% raw %}{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 5 minutes.{% endraw %}"
      summary: "{% raw %}Instance {{ $labels.instance }} down{% endraw %}"
  - alert: SshServiceDown
    expr: "probe_success == 0"
    for: 5m
    labels:
      severity: critical
    annotations:
      message: "{% raw %}{{ $labels.target }} of job {{ $labels.job }} has been down for more than 5 minutes.{% endraw %}"
      summary: "{% raw %}Ssh Service {{ $labels.target }} down{% endraw %}"
  - alert: CriticalCPULoad
    expr: 'instance:node_cpu:load > 98'
    for: 30m
    labels:
      severity: critical
    annotations:
      message: "{% raw %}{{ $labels.instance }} of job {{ $labels.job }} has Critical CPU load for more than 30 minutes.{% endraw %}"
      summary: "{% raw %}Instance {{ $labels.instance }} - Critical CPU load{% endraw %}"
  - alert: CriticalRAMUsage
    expr: 'instance:node_ram:usage > 98'
    for: 5m
    labels:
      severity: critical
    annotations:
      message: "{% raw %}{{ $labels.instance }} has Critical Memory Usage more than 5 minutes.{% endraw %}"
      summary: "{% raw %}Instance {{ $labels.instance }} has Critical Memory Usage{% endraw %}"
  - alert: CriticalDiskSpace
    expr: 'instance:node_fs:disk_space < 20'
    for: 4m
    labels:
      severity: critical
    annotations:
      message: "{% raw %}{{ $labels.instance }} of job {{ $labels.job }} has less than 10% space remaining.{% endraw %}"
      summary: "{% raw %}Instance {{ $labels.instance }} - Critical disk space usage{% endraw %}"
  - alert: ClockSkewDetected
    expr: 'abs(node_timex_offset_seconds) * 1000 > 30'
    for: 2m
    labels:
      severity: warning
    annotations:
      message: "{% raw %}Clock skew detected on {{ $labels.instance }}. Ensure NTP is configured correctly on this host.{% endraw %}"
      summary: "{% raw %}Instance {{ $labels.instance }} - Clock skew detected{% endraw %}"
  ## Disable as Skampi containers continually restart due to unsolved errors - PXH
  # - alert: NumberRestartPerContainer
  #   expr: 'kube_pod_container_status_restarts_total > 30'
  #   for: 1m
  #   labels:
  #     severity: critical
  #   annotations:
  #     description: "{% raw %} Container {{ $labels.container }} of pod
  # {{ $labels.pod }} in namespace
  # {{ $labels.namespace }} keeps restarting.{% endraw %}"
  #     summary: "{% raw %}Container {{ $labels.container }} - Too many restarts{% endraw %}"
  - alert: NumberRestartPerInitContainerContainer
    expr: 'kube_pod_init_container_status_restarts_total > 10'
    for: 1m
    labels:
      severity: critical
    annotations:
      message: "{% raw %} Container {{ $labels.container }} of pod
         {{ $labels.pod }} in namespace
         {{ $labels.namespace }} keeps restarting.{% endraw %}"
      summary: "{% raw %}Container {{ $labels.container }} - Too many restarts{% endraw %}"
  - alert: PendingJobs
    expr: 'gitlab_runner_jobs{state="pending"} > 0'
    for: 10m
    labels:
      severity: critical
    annotations:
      message: "{% raw %}{{ $labels.instance }} of job {{ $labels.job }} has pending jobs for more than 5 minutes..{% endraw %}"
      summary: "{% raw %}Instance {{ $labels.instance }} - Gitlab runner pending jobs{% endraw %}"
  - alert: NumberOfPVStandardClass
    expr: 'sum(kube_persistentvolume_info{storageclass="standard"}) > 0'
    for: 1m
    labels:
      severity: critical
    annotations:
      message: "{% raw %}The Cluster contains more than zero PV of StandardClass{% endraw %}"
      summary: "{% raw %}Presence of Pv type standard class{% endraw %}"
  - alert: FilebeatErrorsFiring
    expr: node_filebeat_firing_logs{level="ERROR"} > 2 # get 2 messages on a reconnect to elastic
    for: 5m
    labels:
      severity: warning
    annotations:
      message: "{% raw %}Filebeat service on node {{ $labels.instance }} has been firing error logs.{% endraw %}"
      summary: "{% raw %}Node {{ $labels.instance }} - Too many filebeat errors.{% endraw %}"
  - alert: FilebeatNoLogs
    expr: node_filebeat_firing_logs{level="INFO"} == 0
    labels:
      severity: warning
    annotations:
      message: "{% raw %}Filebeat service on node {{ $labels.instance }} has not produced any INFO logs.{% endraw %}"
      summary: "{% raw %}Node {{ $labels.instance }} - No filebeat logs produced.{% endraw %}"
  - alert: FilebeatMetricsNotUpdated
    expr: time() - node_filebeat_metrics_update_seconds > 120
    labels:
      severity: warning
    annotations:
      message: "{% raw %}Filebeat service metrics on node {{ $labels.instance }} have not been updated in the last 2 minutes.{% endraw %}"
      summary: "{% raw %}Node {{ $labels.instance }} - Filebeat metrics not updated.{% endraw %}"
prometheus_k8s_external_dns_entry: k8s.stfc.skao.int
