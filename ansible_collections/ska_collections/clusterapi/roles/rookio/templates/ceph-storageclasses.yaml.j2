---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfss1
provisioner: rook-ceph.cephfs.csi.ceph.com
parameters:
  # clusterID is the namespace where operator is deployed.
  clusterID: "rook-ceph"

  # CephFS filesystem name into which the volume shall be created
  fsName: "cephfs"

  # Ceph pool into which the volume shall be created
  # Required for provisionVolume: "true"
  pool: "cephfs_data"

  # Root path of an existing CephFS volume
  # Required for provisionVolume: "false"
  #provisionVolume: "false"
  # rootPath: /shared

  # The secrets contain Ceph admin credentials. These are generated automatically by the operator
  # in the same namespace as the cluster.
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: {{ rook_namespace }} # namespace:cluster
  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
  csi.storage.k8s.io/controller-expand-secret-namespace: {{ rook_namespace }} # namespace:cluster
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
  csi.storage.k8s.io/node-stage-secret-namespace: {{ rook_namespace }} # namespace:cluster

  # (optional) The driver can use either ceph-fuse (fuse) or ceph kernel client (kernel)
  # If omitted, default volume mounter will be used - this is determined by probing for ceph-fuse
  # or by setting the default mounter explicitly via --volumemounter command-line argument.
  # mounter: kernel
  reclaimPolicy: Delete
  allowVolumeExpansion: "true"
  # mountOptions:
    # uncomment the following line for debugging
    #- debug

---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfs
provisioner: rook-ceph.cephfs.csi.ceph.com
parameters:
  # clusterID is the namespace where operator is deployed.
  clusterID: "{{ rook_namespace }}"

  # CephFS filesystem name into which the volume shall be created
  fsName: "cephfs"

  # Ceph pool into which the volume shall be created
  # Required for provisionVolume: "true"
  pool: "cephfs_data"

  # Root path of an existing CephFS volume
  # Required for provisionVolume: "false"
  #provisionVolume: "false"
  # rootPath: /shared

  # The secrets contain Ceph admin credentials. These are generated automatically by the operator
  # in the same namespace as the cluster.
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: {{ rook_namespace }} # namespace:cluster
  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
  csi.storage.k8s.io/controller-expand-secret-namespace: {{ rook_namespace }} # namespace:cluster
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
  csi.storage.k8s.io/node-stage-secret-namespace: {{ rook_namespace }} # namespace:cluster

  # (optional) The driver can use either ceph-fuse (fuse) or ceph kernel client (kernel)
  # If omitted, default volume mounter will be used - this is determined by probing for ceph-fuse
  # or by setting the default mounter explicitly via --volumemounter command-line argument.
  # mounter: kernel
  reclaimPolicy: Delete
  allowVolumeExpansion: "true"
  # mountOptions:
    # uncomment the following line for debugging
    #- debug

---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: bds1
  namespace: {{ rook_namespace }}
provisioner: rook-ceph.rbd.csi.ceph.com
parameters:
  # clusterID is the namespace where the rook cluster is running
  # If you change this namespace, also change the namespace below where the secret namespaces are defined
  clusterID: "{{ rook_namespace }}"

  # If you want to use erasure coded pool with RBD, you need to create
  # two pools. one erasure coded and one replicated.
  # You need to specify the replicated pool here in the 'pool' parameter, it is
  # used for the metadata of the images.
  # The erasure coded pool must be set as the 'dataPool' parameter below.
  #dataPool: ec-data-pool
  pool: "volumes"

  # RBD image format. Defaults to "2".
  imageFormat: "2"

  # RBD image features. Available for imageFormat: "2". CSI RBD currently supports only 'layering' feature.
  imageFeatures: layering

  # The secrets contain Ceph admin credentials. These are generated automatically by the operator
  # in the same namespace as the cluster.
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: {{ rook_namespace }} # namespace:cluster
  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/controller-expand-secret-namespace: {{ rook_namespace }} # namespace:cluster
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
  csi.storage.k8s.io/node-stage-secret-namespace: {{ rook_namespace }} # namespace:cluster
  # Specify the filesystem type of the volume. If not specified, csi-provisioner
  # will set default as 'ext4'.
  csi.storage.k8s.io/fstype: "ext4"
  # uncomment the following to use rbd-nbd as mounter on supported nodes
  #mounter: rbd-nbd
  allowVolumeExpansion: "true"
  reclaimPolicy: "Delete"
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: block
  namespace: {{ rook_namespace }}
provisioner: rook-ceph.rbd.csi.ceph.com
parameters:
  # clusterID is the namespace where the rook cluster is running
  # If you change this namespace, also change the namespace below where the secret namespaces are defined
  clusterID: "{{ rook_namespace }}"

  # If you want to use erasure coded pool with RBD, you need to create
  # two pools. one erasure coded and one replicated.
  # You need to specify the replicated pool here in the 'pool' parameter, it is
  # used for the metadata of the images.
  # The erasure coded pool must be set as the 'dataPool' parameter below.
  #dataPool: ec-data-pool
  pool: "volumes"

  # RBD image format. Defaults to "2".
  imageFormat: "2"

  # RBD image features. Available for imageFormat: "2". CSI RBD currently supports only 'layering' feature.
  imageFeatures: layering

  # The secrets contain Ceph admin credentials. These are generated automatically by the operator
  # in the same namespace as the cluster.
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: {{ rook_namespace }} # namespace:cluster
  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/controller-expand-secret-namespace: {{ rook_namespace }} # namespace:cluster
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
  csi.storage.k8s.io/node-stage-secret-namespace: {{ rook_namespace }} # namespace:cluster
  # Specify the filesystem type of the volume. If not specified, csi-provisioner
  # will set default as 'ext4'.
  csi.storage.k8s.io/fstype: "ext4"
  # uncomment the following to use rbd-nbd as mounter on supported nodes
  #mounter: rbd-nbd
  allowVolumeExpansion: "true"
  reclaimPolicy: "Delete"

---
apiVersion: ceph.rook.io/v1
kind: CephBlockPool
metadata:
  name: volumes
  namespace: {{ rook_namespace }}
spec:
  failureDomain: host
  replicated:
    size: 1
